{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bc2fb428",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0027f0b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = os.path.split(os.path.realpath('__file__'))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9ff6cd2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#reading dictionary of words \n",
    "with open(os.path.join(filepath,'Tagalog Words.txt'), 'r', encoding='utf-8') as f:\n",
    "    word_list = [words.strip() for words in f.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4134cd53",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to calculate Levenshtein Distance\n",
    "dist_dict = {}\n",
    "\n",
    "def LevDistance(orig_string, corr_string):\n",
    "    '''\n",
    "    Function to calculate Levenshtein Distance\n",
    "    orig_string : Original string which needs to be corrected\n",
    "    corr_string : Potential corrected version of the string\n",
    "    '''\n",
    "\n",
    "    if min(len(orig_string), len(corr_string)) == 0:\n",
    "        #If one either of the string is empty, distance is the length\n",
    "        #of the non-empty string\n",
    "        return max(len(orig_string), len(corr_string))\n",
    "    elif (orig_string==corr_string):\n",
    "        #If both the strings are exactly same, distance is zero\n",
    "        return 0\n",
    "    \n",
    "    if orig_string[-1] == corr_string[-1]:\n",
    "        cost = 0\n",
    "    else:\n",
    "        cost = 1\n",
    "#     print(orig_string,\" + \",corr_string, \" : cost - \",str(cost))   \n",
    "    \n",
    "    l1 = (orig_string[:-1], corr_string)\n",
    "    if not l1 in dist_dict:\n",
    "        dist_dict[l1] = LevDistance(*l1)\n",
    "    \n",
    "    l2 = (orig_string, corr_string[:-1])\n",
    "    if not l2 in dist_dict:\n",
    "        dist_dict[l2] = LevDistance(*l2)\n",
    "\n",
    "    l3 = (orig_string[:-1], corr_string[:-1])\n",
    "    if not l3 in dist_dict:\n",
    "        dist_dict[l3] = LevDistance(*l3)\n",
    "        \n",
    "    res = min([dist_dict[l1]+1, dist_dict[l2]+1, dist_dict[l3]+cost])\n",
    "        \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7bc4c51c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LevDistance(\"Babuy\",\"Baboy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a347d380",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "# from edit_distance import EditDistance\n",
    "import re\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "87560691",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpellChecker:\n",
    "    def __init__(self,text_path):\n",
    "        dir_name = os.path.dirname(os.path.realpath('__file__'))\n",
    "        self.dictionary_file = dir_name+\"/Tagalog Words.txt\"\n",
    "        self.corpus_file = dir_name+text_path\n",
    "        pass\n",
    "\n",
    "    def read_dictionary(self):\n",
    "        self.dictionary_words = set(line.strip() for line in open(self.dictionary_file))\n",
    "        \n",
    "    def corpus_word_freq(self, print_results=False, ignore_higfreq_words = True, eval_lines =50):\n",
    "        '''\n",
    "        Function to generate dictionary for correct and corrupted words in the corpus.\n",
    "        \n",
    "        Generating dictionary for correct words in the corpus by comparing if they\n",
    "        exists in the available dictionary 'corpus_word_dict' of words and if they \n",
    "        don't, adding them the to the dictionary 'invalid_corpus_tokens' of \n",
    "        corrupted words.\n",
    "        '''\n",
    "        # initiating dictionary for words\n",
    "        self.corpus_word_dict = {}\n",
    "        self.invalid_corpus_tokens = {}\n",
    "        \n",
    "        lines_eval = 0\n",
    "        \n",
    "        with open(self.corpus_file,'r') as f:\n",
    "            for line in f:\n",
    "                word_list = re.findall(r'\\b[a-zA-Z]+\\b', line)\n",
    "                for word in word_list:\n",
    "                    #checking if the word exists in the given dictionary\n",
    "                    if word.lower() in self.dictionary_words:\n",
    "                        self.corpus_word_dict[word.lower()] = self.corpus_word_dict.get(word.lower(), 0) + 1\n",
    "                    # if it doesn't exists, assuming it to be corrupted word and adding to other dictionary  \n",
    "                    # However, if the words start with a Upper case alphabet, assuming it to be a proper noun and \n",
    "                    # ignoring those words to be added to corrupted dict\n",
    "                    elif(lines_eval < eval_lines and not (word[0].isupper() and word[1].islower())):    \n",
    "                        self.invalid_corpus_tokens[word.lower()] = self.invalid_corpus_tokens.get(word.lower(),0) + 1\n",
    "                lines_eval += 1        \n",
    "\n",
    "        '''\n",
    "        Since some words won't be the part of correct words dict, and if a word \n",
    "        in corrupted dict appears over a certain number of times, it is \n",
    "        considered as not corrupted and moved to the correct word dict.\n",
    "        '''\n",
    "        if ignore_higfreq_words:                \n",
    "            self.ignored_words = set()    \n",
    "            for key,val in self.invalid_corpus_tokens.items():\n",
    "                if int(val) > 25:\n",
    "                    self.ignored_words.add(key)\n",
    "            for word in self.ignored_words:\n",
    "                self.corpus_word_dict[word] = self.invalid_corpus_tokens[word]\n",
    "                del self.invalid_corpus_tokens[word]\n",
    "            \n",
    "        if print_results:\n",
    "            print(str(len(self.corpus_word_dict)),\" correct words added to the corpus dictionary\")\n",
    "            print(str(len(self.invalid_corpus_tokens)),\" corrupted words exists in the corpus\")\n",
    "\n",
    "        \n",
    "    def print_top_n_line(self, n = 50, ignore_higfreq_words= True, calc_word_dict = True):\n",
    "        '''\n",
    "        Function to print first n lines from the corpus with underlined \n",
    "        corrupted words\n",
    "        '''\n",
    "        line_count = 0\n",
    "        if calc_word_dict:\n",
    "            self.corpus_word_freq(ignore_higfreq_words = ignore_higfreq_words, eval_lines=n)\n",
    "            \n",
    "        with open(self.corpus_file, 'r') as f:\n",
    "            for line in f:\n",
    "                word_list = re.findall(r'\\b[a-zA-Z]+\\b', line)\n",
    "                for word in word_list:\n",
    "                    if word.lower() in self.invalid_corpus_tokens:\n",
    "                        line = re.sub(word,\"\\033[4m\"+word+\"\\033[0m\",line)\n",
    "                print(line)\n",
    "                line_count += 1\n",
    "                if line_count >= n:\n",
    "                    break\n",
    "        \n",
    "        \n",
    "    def closest_replacement(self, possible_opt_dict):\n",
    "        '''\n",
    "        Function to resolve conflict in case of multiple replacements\n",
    "        with same edit distance available.\n",
    "        Replacement with the max occurance in the corpus will be selected\n",
    "        '''\n",
    "        inv_dict = {}\n",
    "        for key, value in possible_opt_dict.items():\n",
    "            if key in self.corpus_word_dict.keys():\n",
    "                possible_opt_dict[key] = self.corpus_word_dict[key]\n",
    "                inv_dict[possible_opt_dict[key]] = key\n",
    "        if len(inv_dict) > 0 :\n",
    "            return inv_dict[max(inv_dict.keys())]\n",
    "        else:\n",
    "            return None \n",
    "        \n",
    "        \n",
    "    def spell_check_first_n_lines(self, n=50, ignore_higfreq_words=True, min_distance = 4, \n",
    "                                  show_status = False, show_options = True, print_results = False):\n",
    "        '''\n",
    "        Function to spell check first n lines and replace the corrupted\n",
    "        words by the closest replacement in the dictionary. \n",
    "        \n",
    "        Closest replacement of the word is defined using Levenshtein Distance\n",
    "        between the corrupted word and the words in dictonary. If multiple \n",
    "        replacement words with shortest distance are found, the replacement option\n",
    "        with the highest freqeuncy in the corpus is used. \n",
    "        \n",
    "        Output:- Output of the function will be saved in \"Output.txt\" file in the current directory\n",
    "        \n",
    "        n : Number of lines to be evaluated\n",
    "        ignore_higfreq_words : default:- True; Flag to ignore high frequency word from corrupted word dict\n",
    "        min_distance : default :- 4; Min edit distance to be used. \n",
    "        show_status  : defualt :- False; Flag to enable printing of status with every correction\n",
    "        show_options : default :- True; Flag to enable printing of options for each corrupted words\n",
    "        print_results: default :- False; Flag to enable priting of the corrected text after spell check\n",
    "        '''\n",
    "        \n",
    "        self.corpus_word_freq(ignore_higfreq_words = ignore_higfreq_words, eval_lines = n)\n",
    "        self.replacement_dict = {}\n",
    "        for index, invalid_word in enumerate(self.invalid_corpus_tokens):\n",
    "            if show_status:\n",
    "                print(\"Processing word \"+str(index)+\" of \"+str(len(self.invalid_corpus_tokens))+\" word \"+invalid_word)\n",
    "            distance_dict = {}\n",
    "            min_dist = min_distance\n",
    "            for valid_word in self.dictionary_words:\n",
    "                # Only calculating edit distance with the correct words whose lenght is +/- 1 length of \n",
    "                # corrupted words\n",
    "                if (len(valid_word) >= len(invalid_word)-1) and (len(valid_word) <= len(invalid_word)+1):\n",
    "                    #edit_dist = EditDistance(invalid_word, valid_word).calculate()\n",
    "                    edit_dist = LevDistance(valid_word,invalid_word)\n",
    "                    if edit_dist < min_dist:\n",
    "                        min_dist = edit_dist\n",
    "                        distance_dict = {}\n",
    "                    if edit_dist <= min_dist: \n",
    "                        distance_dict[valid_word] =  edit_dist\n",
    "                        \n",
    "            #print(distance_dict)\n",
    "            # In case only one replacement found for the corrupted word with the minimum possible distance\n",
    "            # then, it will be selected as the replacement option.\n",
    "            if len(distance_dict) == 1:\n",
    "                self.replacement_dict[invalid_word] = list(distance_dict.keys())[0]\n",
    "            # In case multiple replacement options are available with the minimum edit distance, the word \n",
    "            # having highest frequency in corpus will be selected as the replacement option.    \n",
    "            elif len(distance_dict) > 1 and self.closest_replacement(distance_dict) is not None:\n",
    "                self.replacement_dict[invalid_word] = self.closest_replacement(distance_dict)\n",
    "            else:    \n",
    "            # In case no replacement word is found within the minimum distance limit, the corrupted word \n",
    "            # will be assigned as the replacement option.    \n",
    "                self.replacement_dict[invalid_word] = invalid_word\n",
    "         \n",
    "        if show_options:\n",
    "            print(\"Found the following replacements : \")\n",
    "            print(self.replacement_dict)\n",
    "        \n",
    "        self.correct_text(replace_lines = n, print_res=print_results)\n",
    "               \n",
    "\n",
    "    def correct_text(self, replace_lines = 50, print_res=False):\n",
    "        line_count = 0\n",
    "        os.remove(\"Output.txt\")\n",
    "        print(\"\\n\")\n",
    "        with open(self.corpus_file, 'r') as f:\n",
    "            for line in f:\n",
    "                word_list = re.findall(r'\\b[a-zA-Z]+\\b', line)\n",
    "                for word in word_list:\n",
    "                    if word.lower() in self.invalid_corpus_tokens:\n",
    "                        line = re.sub(word,self.replacement_dict[word.lower()],line)\n",
    "                if print_res:\n",
    "                    print(line)    \n",
    "                with open('Output.txt', 'a') as w:        \n",
    "                    print(line, file=w)\n",
    "                line_count += 1\n",
    "                if line_count >= replace_lines:\n",
    "                    print(\"\\n---Corrected text written to Output.txt---\")\n",
    "                    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9f90932f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = SpellChecker(\"/test.txt\")\n",
    "sc.read_dictionary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "75fe8786",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[4mbabuy\u001b[0m\n",
      "\n",
      "\u001b[4mkamates\u001b[0m\n",
      "\n",
      "\u001b[4mseboyas\u001b[0m\n",
      "\n",
      "\u001b[4mbabai\u001b[0m\n",
      "\n",
      "\u001b[4msanggul\u001b[0m\n",
      "\n",
      "\u001b[4msekwinta\u001b[0m\n",
      "\n",
      "\u001b[4msalamen\u001b[0m\n",
      "\n",
      "\u001b[4mbuhuk\u001b[0m\n",
      "\n",
      "\u001b[4msengkamas\u001b[0m\n",
      "\n",
      "\u001b[4mtalung\u001b[0m\n",
      "\n",
      "\u001b[4msarewa\u001b[0m\n",
      "\n",
      "\u001b[4mkalekasan\u001b[0m\n",
      "\n",
      "\u001b[4mpangaku\u001b[0m\n",
      "\n",
      "\u001b[4mgoneta\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "sc.print_top_n_line(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "81eaea2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.corpus_word_freq(eval_lines=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cb247e2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing word 0 of 14 word babuy\n",
      "Processing word 1 of 14 word kamates\n",
      "Processing word 2 of 14 word seboyas\n",
      "Processing word 3 of 14 word babai\n",
      "Processing word 4 of 14 word sanggul\n",
      "Processing word 5 of 14 word sekwinta\n",
      "Processing word 6 of 14 word salamen\n",
      "Processing word 7 of 14 word buhuk\n",
      "Processing word 8 of 14 word sengkamas\n",
      "Processing word 9 of 14 word talung\n",
      "Processing word 10 of 14 word sarewa\n",
      "Processing word 11 of 14 word kalekasan\n",
      "Processing word 12 of 14 word pangaku\n",
      "Processing word 13 of 14 word goneta\n",
      "\n",
      "\n",
      "babuy\n",
      "\n",
      "kamatis\n",
      "\n",
      "seboyas\n",
      "\n",
      "babai\n",
      "\n",
      "sanggol\n",
      "\n",
      "sekwinta\n",
      "\n",
      "salamin\n",
      "\n",
      "buhok\n",
      "\n",
      "singkamas\n",
      "\n",
      "talung\n",
      "\n",
      "sarewa\n",
      "\n",
      "kalekasan\n",
      "\n",
      "pangako\n",
      "\n",
      "goneta\n",
      "\n",
      "\n",
      "Execution time - 9.354 seconds\n"
     ]
    }
   ],
   "source": [
    "starttime = time.time()\n",
    "sc.spell_check_first_n_lines(500, show_status=True,show_options=False, print_results= True)\n",
    "print(\"\\n\\nExecution time - %s seconds\"% round(time.time()-starttime,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5253fefc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
